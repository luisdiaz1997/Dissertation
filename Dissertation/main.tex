% Stanford University PhD thesis style -- modifications to the report style
% This is unofficial so you should always double check against the
% Registrar's office rules
% See http://library.stanford.edu/research/bibliography-management/latex-and-bibtex
% 
% Example of use below
% See the suthesis-2e.sty file for documentation
%
\documentclass{report}
\usepackage{suthesis-2e}
\usepackage{amsmath}
\usepackage{amssymb}
\dept{BioPhysics}

\begin{document}
\title{Gaussian Processes\\
            for Spatial Genomics}
\author{Luis Fernando Chumpitaz Diaz}
\principaladviser{Barbara Engelhardt}
\firstreader{Barbara Engelhardt}
\secondreader{Julia Palacios}
\thirdreader{Scott W Linderman} %if needed
\fourthreader{Trevor Hastie} %if needed
 
\beforepreface
\prefacesection{Preface}
This thesis tells you all you need to know about...
\prefacesection{Acknowledgments}
I would like to thank...
\afterpreface

\chapter{Introduction}
An overview of the motivation for leveraging Gaussian Processes in spatial genomics, highlighting the common thread between the three research thrusts developed in this dissertation.

\chapter{Background on Gaussian Processes}
\section{Standard Gaussian Processes}
Gaussian Processes (GPs) define distributions over latent functions $f : \mathcal{X}\rightarrow \mathbb{R}$ such that any finite collection of function values follows a multivariate normal distribution. A GP prior is characterized by a mean function $m(x)$ and covariance kernel $k(x, x')$ that encode smoothness, symmetries, and expected variability. Kernels such as the squared-exponential produce infinitely differentiable functions, whereas Matérn kernels allow controlled roughness and sharper spatial transitions that better match tissue boundaries. Conditioning on observed data yields closed-form posterior means and covariances, enabling principled interpolation with calibrated uncertainty. The cubic complexity of exact GP inference ($\mathcal{O}(N^{3})$ factorization and $\mathcal{O}(N^{2})$ storage) becomes prohibitive for the hundreds of thousands of observations that are routine in spatial genomics, motivating approximate but scalable methods.

\section{Variational Gaussian Processes and Sparse Approximations}


\subsection{Variational Gaussian Processes and Inducing Points} 
(SVGPs) \cite{Hensman2013-xr} introduce a set of $M$ inducing variables $U$ at inducing inputs $Z$ to summarize the latent function, reducing complexity to $\mathcal{O}(NM^{2})$. A variational distribution $q(U)$ is optimized to maximize an evidence lower bound (ELBO) that balances data fit with a KL divergence to the GP prior. This framework enables stochastic optimization and mini-batching, making it suitable for large datasets. However, SVGPs typically assume a Gaussian variational family with diagonal covariance, which can underestimate posterior uncertainty when the true posterior exhibits strong correlations. 

\subsection{Variational Free Energy Gaussian Processes}
\label{sec:vfe-background}
Classical sparse variational GP formulations frequently assume diagonal or block-diagonal forms for $q(U)$, discarding cross-group correlations even when the prior couples groups strongly. Variational nearest-neighbor GPs (VNNGPs) lower computational cost by conditioning on spatial neighborhoods, yet they encode dependence solely through Euclidean distance and therefore fail to leverage biological relationships such as developmental trajectories or treatment/control pairings. Separable MGGP kernels exacerbate the problem by forcing every group to share an identical spatial correlation envelope, leading to oversmoothed posteriors and poor handling of sharp group-specific differences. These limitations motivate the kernel and inference techniques developed in Chapter~\ref{chap:variational-mggp}, so here we summarize the variational free-energy machinery that underpins those methods.

\subsubsection{Scaling Variational MGGPs}
While the MGGP variational framework enables stable training for our NSF model, in practice we are constrained by memory and can only use on the order of \(M{=}3{,}000\) inducing points during training—similar to the original NSF memory profile \cite{Townes2023-it}. 

\subsubsection*{Variational Nearest Neighbor Gaussian Processes}
VNNGPs offer an approximation of SVGP by using K-neast neighbors. In specfic a precision approximation for the inducing GP prior \cite{Wu2022-fd}.

\begin{align*}
p(U) &= p(U_1) \prod_{j=2}^{M} p(U_j | U_{1, \dots, j-1})\\
&\approx  \prod_{j=1}^{M} p(U_j | U_{n(j)})
\end{align*}

And they use the following approximation of the inducing variational distribution.
\begin{align*}
q(u) &= \prod_{i}^{M}\mathcal{N}\!\big(u_i; m_{u_i}, s_{u_i}\big),
\end{align*}

\subsubsection{Evidence Lower Bound}
The variational lower bound (ELBO) for the SVGP is:
\begin{align}
\log p(y;\theta) 
&\geq \mathcal{L}(q) \\
&= \mathbb{E}_{q(f)}\!\left[\log p(y \mid f)\right]
  - \mathrm{KL}\!\big[q(u)\,\|\, p(u)\big].
\end{align}

Here, \(q(u)\) is the variational distribution over inducing variables, 
\(p(u)\) is their GP prior, and the expectation is with respect to the 
induced marginal \(q(f)\).
In general the first expectation term can be calculated using Monte Carlo sampling.

\paragraph{SVGP-implied moments and notation.}
Let \(X=\{x_i\}_{i=1}^N\) be inputs, \(Z=\{z_m\}_{m=1}^M\) inducing locations,
\(K_{ff} \in \mathbb{R}^{N\times N}\), \(K_{uu}\in\mathbb{R}^{M\times M}\),
\(K_{fu}\in\mathbb{R}^{N\times M}\) and \(K_{uf}=K_{fu}^\top\).
Assume \(q(u)=\mathcal{N}(m_u,S_u)\) with \(m_u\in\mathbb{R}^{M}\), \(S_u\in\mathbb{R}^{M\times M}\).
For standard SVGP,
\begin{align}
m_f &= K_{fu}K_{uu}^{-1} m_u,\\
\Sigma_f &= K_{ff} - Q_{ff} + K_{fu}K_{uu}^{-1} S_u K_{uu}^{-1}K_{uf},
\qquad
Q_{ff} := K_{fu}K_{uu}^{-1}K_{uf}.
\end{align}
The (one-point) predictive marginal at a test input \(x_i\) is
\(q(f;x_i)=\mathcal{N}\!\big(m_f(x_i),\, \Sigma_f(x_i,x_i)\big)\).
This is the \emph{marginal} at \(x_i\); jointly, the vector \(f=(f_1,\ldots,f_N)\) is
\(\mathcal{N}(m_f,\Sigma_f)\) and is generally \emph{not} factorized across inputs.

The samples can be reparameterized as
\[
\hat{f}_i = m_f(x_i) + \Sigma_f(x_i,x_i)^{1/2}\,\epsilon,\quad \epsilon \sim \mathcal{N}(0,1).
\]

\subsubsection{Closed-Form Expected Log-Likelihood}
When the likelihood is Gaussian noise,
\(y = f + \sigma \epsilon\) with \(\epsilon\sim\mathcal{N}(0,I)\),
so \(p(y\mid f) = \mathcal{N}(f, \sigma^2 I)\).
Then
\begin{align*}
\mathbb{E}_{q(f)}\!\left[\log p(y \mid f)\right]  
&= \mathbb{E}_{q(f)}\!\left[ - \tfrac{1}{2\sigma^2}(y-f)^\top(y-f) \right] 
    - \tfrac{N}{2}\log(2\pi\sigma^2).
\end{align*}
For the quadratic term,
\begin{align*}
    \mathbb{E}\!\left[ (y-f)^\top(y-f)\right] 
    &= y^\top y - 2 \mathbb{E}[f]^\top y + \mathbb{E}[f^\top f]\\
    &= y^\top y - 2 \mathbb{E}[f]^\top y + \mathbb{E}\!\big[(f-\mathbb{E}[f])^\top(f-\mathbb{E}[f])\big] + \mathbb{E}[f]^\top\mathbb{E}[f]\\
    &=  \big(y-\mathbb{E}[f]\big)^\top\!\big(y-\mathbb{E}[f]\big) + \mathrm{Tr}\!\big(\mathrm{Cov}(f)\big)\\
    &=   (y-m_f)^\top(y-m_f) + \mathrm{Tr}(\Sigma_f).
\end{align*}
Therefore,
\begin{align*}
\mathbb{E}_{q(f)}\!\left[\log p(y \mid f)\right]  
&= - \frac{1}{2\sigma^2}\Big[(y-m_f)^\top(y-m_f) + \mathrm{Tr}(\Sigma_f)\Big]
   - \frac{N}{2}\log(2\pi\sigma^2)\\
&= \log \mathcal{N}\!\big(y \,;\, m_f, \sigma^2 I\big) \; - \; \frac{1}{2\sigma^2}\mathrm{Tr}(\Sigma_f),
\end{align*}
i.e.,
\begin{align}
\mathbb{E}_{q(f)}[\log p(y \mid f)]
= -\tfrac{N}{2}\log(2\pi\sigma^2)
-\tfrac{1}{2\sigma^2}\Big[ (y-m_f)^\top(y-m_f) + \mathrm{tr}(\Sigma_f)\Big].
\end{align}
The trace term sums the marginal variances contributed by uncertainty under \(q(f)\).

\subsubsection{Closed-Form KL Term}
With \(q(u)=\mathcal{N}(m_u,S_u)\) and the GP prior \(p(u)=\mathcal{N}(0,K_{uu})\),
\begin{align}
\mathrm{KL}\!\big[q(u)\,\|\,p(u)\big]
= \tfrac{1}{2}\Big(
\log\tfrac{|K_{uu}|}{|S_u|}
 - M
 + \mathrm{tr}\big(K_{uu}^{-1} S_u\big)
 + m_u^\top K_{uu}^{-1} m_u
\Big),
\end{align}
where $M=\dim(u)$.
This term is exact and independent of the data \(y\).

\subsection{Stochastic Variational Gaussian Processes}

VFE-GPs provide a tractable solution to the ELBO in Gaussian Processes when the likelihood is also Gaussian. The ELBO can be computed in closed form, allowing for efficient optimization of the variational parameters. However, when the likelihood is non-Gaussian, the expected log-likelihood term in the ELBO becomes intractable. 


\subsection{Variational Nearest Neighbor Gaussian Prcesses}
Variational Nearest-Neighbor GPs (VNNGPs) \cite{Wu2022-fd} further reduce complexity by conditioning each point on a local neighborhood of $K$ nearest neighbors, achieving near-linear scaling in $N$. VNNGPs preserve excellent accuracy for single-group spatial data but rely exclusively on spatial proximity to define dependencies, ignoring structured relationships between biological groups. This motivates the development of new kernels and inference techniques that can capture both spatial and group-level structure, as described in Chapter~\ref{chap:variational-mggp}.
Scalable GP inference introduces inducing variables $U$ at inducing inputs $Z$ to summarize the latent function while keeping $|Z| = M \ll N$. Variational sparse GPs posit a tractable posterior $q(U)$ and optimize an evidence lower bound (ELBO) that balances data fit with a KL divergence to the GP prior, enabling stochastic updates and $\mathcal{O}(NM^{2})$ complexity \cite{Hensman2013-xr}. Variational Nearest-Neighbor GPs (VNNGPs) push this idea further by conditioning each point on a local neighborhood, reducing costs to near-linear in $N$ while preserving excellent accuracy for single-group spatial data. However, VNNGPs still rely exclusively on spatial proximity when deciding conditional dependencies and therefore ignore structured relationships between biological groups, a deficiency addressed in Chapter~\ref{chap:variational-mggp}.

\section{Multi-Group Gaussian Processes}
Spatial genomics experiments capture multiple related groups (cell types, tissue regions, disease states), requiring latent functions $f(x, c)$ that depend on spatial coordinates $x$ and group index $c$. Classical multi-group GP (MGGP) models employ separable kernels $k\big((x, c), (x', c')\big) = k_{\text{space}}(x, x')\,k_{\text{group}}(c, c')$, often choosing RBF kernels for both factors. Separable RBF kernels share the same spatial correlation envelope across groups, which oversmooths sharp group-specific boundaries and fails to capture heterogeneous spatial roughness. Spatial transcriptomics demands finer control over smoothness; Matérn kernels provide explicit parameters for roughness ($\nu$) and length scales ($\ell$), motivating the non-separable Matérn MGGP kernel introduced in Chapter~\ref{chap:variational-mggp}. That kernel decouples spatial smoothness from cross-group similarity, enabling credible uncertainty estimates even when groups exhibit distinct spatial signatures.

\chapter{Variational Inference for Multi-Group Gaussian Processes}\label{chap:variational-mggp}

\section{Stochastic Variational Multigroup Gaussian Processes}

In order to perform inference using Multigroup Gaussian Processes, we first need to build a variational strategy for such Gaussian Processes for any non-gaussian likelihood.
We follow similar strategies of previous stochastic variational Gaussian processes methods (SVGP) \cite{Hensman2013-xr,Salimbeni2017-ds,Townes2023-it}.

We start by defining the Gaussian Process distribution over $F$ with inputs $X$ and groups $C_X$.
\[p(F |X, C_{X}; \theta) = \mathcal{MGGP}(F \mid 0, K((X, C_{X}), (X, C_{X}))) \]

And the distribution over data $Y$ as the marginal over distribution $F$.
\[p(Y|X, C_{X}; \theta) = \int{p(Y|F) p(F|X, C_{X}; \theta) dF} \]

We can define this Gaussian Process as the marginalization over the inducing point distribution of $U$.
\[p(F|X, C_{X}; \theta) = \int{p(F|U, X, C_{X}, Z, C_{z}; \theta) p(U|Z, C_{z};\theta) dU}\]

This distribution over $U$ is also a Multigroup Gaussian Process, with inducing inputs $Z$ and inducing groups $C_Z$. 
For our implementation, we chose inducing points $Z$ from the distribution of inputs in a given group.

\[ p(U|Z, C_{Z};\theta) = \mathcal{MGGP}(U \mid 0, K((Z, C_{Z}), (Z, C_{Z})))\]

The log marginal then becomes the following:

\[p(Y|X, C_{X}; \theta) = \int{p(Y|F) p(F|U, X, C_{X}, Z, C_{Z}; \theta) p(U|Z, C_{Z};\theta) dU  dF}\]

We add variational distributions $q(F, U)$.
\[\log p(Y|X, C_{X}; \theta) = \log \int{ \frac{p(Y|F) p(F|U, X, C_{X}, Z, C_{Z}; \theta) p(U|Z, C_{Z};\theta)}{q(F, U)} q(F, U) dU  dF}\]

Apply Jensen's Inequality and we obtain the Evidence Lower Bound (ELBO).

\[\log p(Y|X, C_{X}; \theta) \ge  \int{ \log \frac{p(Y|F) p(F|U, X, C_{X}, Z, C_{Z}; \theta) p(U|Z, C_{Z};\theta)}{q(F, U)} q(F, U) dU  dF}\]

We set $q(F|U)=p(F|U)$ \cite{Hensman2013-xr}.

\[q(F, U) = p(F|U, X, C_{X}, Z, C_{Z}; \theta) q(U) \]

This simplifies the ELBO.

\[\log p(Y|X, C_{X}; \theta) \ge  \int{ \log \frac{p(Y|F) p(U|Z, C_{Z};\theta)}{q(U)} q(F, U) dU  dF}\]

\[\log p(Y|X, C_{X}; \theta) \ge  \int{ \log p(Y|F) q(F)} - KL(q(U) ||p(U|Z, C_{Z};\theta))\]

The first Expectation term can be approximated using Monte Carlo sampling if the likelihood is non-Gaussian.

\[\int{\log{p(Y|F)} q(F)dF} \approx \frac{1}{S} \sum^{S}_{s}[\log{p(Y|F_s)}]\]

Where:

\[F_s \sim q(F)\]

Since the variational posterior depend only on the correspoding inputs $(X_i, C_i)$ \cite{Salimbeni2017-ds}.

It allow us to perform inference of $q(F)$ over new combinations of $X$ and $C_X$.

\[q(F| X, C_{X}; \theta) = \int p(F|U, X, C_{X}, Z, C_{Z}; \theta) q(U) dU \]
\[q(F| X_{new}, C_{X_{new}}; \theta) = \int p(F|U, X_{new}, C_{X_{new}}, Z, C_{Z}; \theta) q(U) dU \]

\section{Limitations of Existing Approaches}
Section~\ref{sec:vfe-background} establishes the SVGP evidence lower bound, the closed-form moment expressions, and the VNNGP precision approximation that we build upon in this chapter. We now focus on how those ingredients behave in multi-group models under the simplifying choice $X=Z$, which allows the ELBO to be written entirely in terms of the variational parameters.

Taking $X=Z$ foregrounds the tension between the trace penalties in the likelihood term and the structural assumptions placed on $S_u$ (e.g., diagonal vs.\ full-rank covariances). The following analysis recalls the expressions from Section~\ref{sec:vfe-background} and uses them to diagnose where diagonal variational forms and jittered kernels collapse uncertainty in MGGP settings.

\subsubsection{Effect of $X=Z$ and diagonal vs.\ full $S_u$ on the ELBO}

\paragraph{When $X=Z$, the variational marginal matches $q(u)$.}
If $X=Z$ (and we use the same kernel for $f$ and $u$), then
\[
K_{ff}=K_{uu},\qquad K_{fu}=K_{uu},\qquad Q_{ff}=K_{uu}.
\]
Hence, from the standard SVGP moments,
\[
m_f \;=\; K_{fu}K_{uu}^{-1}m_u \;=\; m_u,\qquad
\Sigma_f \;=\; K_{ff}-Q_{ff}+K_{fu}K_{uu}^{-1}S_uK_{uu}^{-1}K_{uf} \;=\; S_u.
\]
Therefore, under $X=Z$ the induced marginal over $f$ is exactly $q(f)=\mathcal{N}(m_u,S_u)$.

\paragraph{Isolating the $S_u$-dependent part of the ELBO (Gaussian likelihood).}
For isotropic Gaussian noise $p(y\mid f)=\mathcal{N}(f,\sigma^2 I)$, the expected log-likelihood
contributes $-\tfrac{1}{2\sigma^2}\operatorname{tr}(\Sigma_f)$, cf.\ Eq.\ (5).
With $X=Z$ we have $\Sigma_f=S_u$, so the $S_u$-dependent part of the ELBO is
\[
\mathcal{L}(S_u) \;=\; \underbrace{\tfrac{1}{2}\log|S_u| - \tfrac{1}{2}\operatorname{tr}(K_{uu}^{-1}S_u)}_{\textstyle -\mathrm{KL}[q(u)\|p(u)]~\text{terms in }S_u}
\;-\; \underbrace{\tfrac{1}{2\sigma^2}\operatorname{tr}(S_u)}_{\textstyle 
\mathbb{E}\left[\log p(y\!\mid\! f)\right]
~\text{term}}
\;+\; \text{const},
\]
and we maximize $\mathcal{L}(S_u)$ w.r.t.\ $S_u\succ 0$.

\paragraph{Full-covariance optimum.}
Taking the derivative and setting it to zero gives
\[
\frac{\partial \mathcal{L}}{\partial S_u}
\;=\; \tfrac{1}{2}S_u^{-1} - \tfrac{1}{2}K_{uu}^{-1} - \tfrac{1}{2\sigma^2}I \;=\; 0
\;\Longrightarrow\;
S_u^\star \;=\; \big(K_{uu}^{-1} + \sigma^{-2}I\big)^{-1}.
\]
This is exactly the posterior covariance of $u$ for the GP regression model with $X=Z$ and Gaussian noise, as expected for an unconstrained (full-rank) Gaussian variational family.

\paragraph{Diagonal-constrained optimum.}
If we constrain $S_u=\mathrm{diag}(s_1,\dots,s_M)$, the objective separates:
\[
\mathcal{L}(S_u)
= \sum_{i=1}^M \left[ \tfrac{1}{2}\log s_i - \tfrac{1}{2}(K_{uu}^{-1})_{ii}s_i - \tfrac{1}{2\sigma^2}s_i \right] + \text{const}.
\]
Differentiating each term and setting to zero yields
\[
\frac{1}{2s_i} - \tfrac{1}{2}(K_{uu}^{-1})_{ii} - \tfrac{1}{2\sigma^2} = 0
\quad\Longrightarrow\quad
s_i^\star \;=\; \frac{1}{(K_{uu}^{-1})_{ii} + \sigma^{-2}},\qquad i=1,\dots,M.
\]
Hence
\[
S_{u,\mathrm{diag}}^\star \;=\; \mathrm{diag}\!\left(\frac{1}{(K_{uu}^{-1})_{11} + \sigma^{-2}},\dots,\frac{1}{(K_{uu}^{-1})_{MM} + \sigma^{-2}}\right).
\]

\paragraph{Diagonal variances are \emph{shrunk} relative to the full optimum.}
Let $A := K_{uu}^{-1} + \sigma^{-2}I \succ 0$. The full-covariance optimum has
$S_u^\star = A^{-1}$, while the diagonal optimum has
$S_{u,\mathrm{diag}}^\star = \mathrm{diag}\!\big( 1/A_{11},\dots,1/A_{MM}\big)$.

A standard matrix inequality for $A\succ 0$ states
\[
\big(A^{-1}\big)_{ii} \;\ge\; \frac{1}{A_{ii}}\qquad\text{for all }i,
\]
with strict inequality whenever $A$ has any nonzero off-diagonal entries (equivalently, whenever the prior couples different inducing variables). Thus, the diagonal variational solution \emph{underestimates} the true posterior marginal variances elementwise; informally, it ``pushes'' $S_u$ \emph{toward zero} compared to the full-rank optimum.

\paragraph{Limits and intuition.}
\begin{itemize}
\item As $\sigma^2\!\downarrow 0$, both optima shrink:
\(
S_u^\star = (K_{uu}^{-1}+\sigma^{-2}I)^{-1} \to 0
\)
and
\(
S_{u,\mathrm{diag}}^\star \to 0
\),
becoming a noiseless GP.
\item For any fixed $\sigma^2>0$, the diagonal constraint removes the ability to encode posterior \emph{correlations}; maximizing the ELBO trades off the concave $\tfrac{1}{2}\log|S_u|$ barrier against the linear penalties
$\tfrac{1}{2}\operatorname{tr}(K_{uu}^{-1}S_u)+\tfrac{1}{2\sigma^2}\operatorname{tr}(S_u)$.
Without access to off-diagonal entries, the optimizer can only increase $\log|S_u|$ by inflating the $s_i$, which is outweighed (coordinatewise) by the linear penalties; hence the elementwise shrinkage.
\end{itemize}

\paragraph{Beyond Gaussian likelihoods.}
For general log-concave likelihoods, the same qualitative effect persists:
the expected log-likelihood penalizes marginal variances via a (typically) negativeterm, while the KL contributes the $\tfrac{1}{2}\log|S_u|$ barrier and linear trace penalties. Constraining $S_u$ to be diagonal eliminates covariance degrees of freedom that would otherwise share variance across dimensions, and the maximizer again exhibits elementwise underestimation relative to the unconstrained optimum.


\subsubsection{Diagonal $S_u$, jitter, and variance collapse}
\label{sec:diag-collapse}

Throughout this section we keep the SVGP notation above and take $X=Z$ so that
$q(f)=\mathcal{N}(m_u,S_u)$ at the data locations. We analyze the effect of
enforcing a \emph{diagonal} variational covariance $S_u=\mathrm{diag}(s_1,\dots,s_M)$
under the common practice of adding a small jitter to the prior kernel
$K_{uu}\leftarrow K_{uu}+\varepsilon I$ with $\varepsilon>0$.

\paragraph{Optimizing only the KL part (likelihood neglected).}
If we ignore the dependence of $\mathbb{E}_{q(f)}[\log p(y\mid f)]$ on the variational
covariance (e.g., $\sigma^2\to\infty$ for a Gaussian likelihood, or simply as an
analytic probe), the $S_u$–dependent objective reduces to
\[
\max_{S_u\succ 0}\quad \tfrac{1}{2}\log|S_u| - \tfrac{1}{2}\mathrm{tr}(K_{uu}^{-1}S_u).
\]
The unconstrained (full) optimum is $S_u^\star=K_{uu}$. Under the \emph{diagonal}
constraint $S_u=\mathrm{diag}(s_1,\ldots,s_M)$, the objective separates and the
stationary conditions are


This works well for standard kernels on 1D inputs (near-banded precisions; Fig.~\ref{fig:precision-sparsity-regular}) but is ill-suited for MGGPs, where cross-group couplings produce non-diagonal block structure in the covariance and hence much denser precisions (Fig.~\ref{fig:precision-sparsity-mggp}). In MGGPs, these types of masking can induce ``separate MGGP'' behavior with ridges/discontinuities \cite{Li2021-mu}.

% \begin{figure}[t]
%   \centering
%   % optional: small padding inside the box

%   \includegraphics[width=0.9\linewidth]{MGGP_NSF_paper/mggp_files/2D_mggps.png}%
  

%   \caption{Covariance matrices of MGGP with 2D inputs at various values of group difference and the sampling of each covariance matrix}
%   \label{fig:precision-sparsity-regular}
% \end{figure}





% \begin{figure}[t]
%   \centering

%   \includegraphics[width=0.9\linewidth]{MGGP_NSF_paper/mggp_files/K400.png}%
  

%   \caption{Covariance matrices of KNN MGGP at K=400 with 2D inputs, and the sampling of each covariance matrix}
%   \label{fig:precision-sparsity-regular}
% \end{figure}

Their variational strategy, reduces to an SVGP with diagonal \(S_u\), leading to underestimated posterior variances  (see Appendix). For our method, it is important to also approximate the non-diagonal variance of the inducing distribution, which pertains to "cell-to-cell" variance, and for MGGP it also captures "group-to-group" variance. 

\section{Derivation of the Non-Separable Matérn MGGP Kernel}
We embed group relationships directly into the spatial covariance with a Matérn form that depends jointly on spatial and inter-group distances. Let $x,x'\in\mathbb{R}^p$ denote spatial coordinates, $c_i,c_j$ group labels, $d_{ij}$ a graph-based group distance, and $a\ge 0$ a scale governing how strongly group separation modulates spatial smoothness. The multigroup Matérn kernel is
\begin{align}
K\!\left((x, c_i), (x', c_j)\right)
&=
\frac{\sigma^2}{\big(a^2 d_{ij}^2 + 1\big)^{\,\nu+\frac{p}{2}}}
\times
\frac{2^{\,1-\nu}}{\Gamma(\nu)}
\times
\left(
\frac{\sqrt{2\nu}\, \|x - x'\|}{\ell\,\sqrt{a^2 d_{ij}^2 + 1}}
\right)^{\!\nu}
K_{\nu}\!\left(
\frac{\sqrt{2\nu}\, \|x - x'\|}{\ell\,\sqrt{a^2 d_{ij}^2 + 1}}
\right),
\label{eq:mggp_matern_kernel}
\end{align}
where $\ell$ and $\sigma^2$ are the base spatial length scale and marginal variance, $\nu$ controls smoothness, and $K_\nu(\cdot)$ is a modified Bessel function of the second kind. The denominator penalizes cross-group pairs in proportion to $d_{ij}$, shrinking both the signal variance and the effective length scale when groups differ. Setting $a=0$ (all groups identical) recovers the usual Matérn kernel, whereas increasing $a$ enlarges cross-group distances and attenuates correlation between dissimilar groups.

\subsection{Reduction to the Standard Matérn}
When $a = 0$ the $(1+a^2 d_{ij}^2)$ factors equal $1$ for all $i,j$, yielding
\begin{equation}
K((x, c_i), (x', c_i)) 
= \sigma^2 \frac{2^{1-\nu}}{\Gamma(\nu)}
\left(\frac{\sqrt{2\nu}\, \|x - x'\|}{\ell}\right)^{\!\nu}
K_{\nu}\!\left(\frac{\sqrt{2\nu}\, \|x - x'\|}{\ell}\right),
\end{equation}
the canonical Matérn kernel with global parameters $(\sigma^2,\ell,\nu)$. Thus the MGGP construction is a strict generalization that defaults to the single-group Matérn when inter-group structure is absent.

\subsection{Closed-Form Special Cases}
Let $r=\|x-x'\|$. For the commonly used half-integer smoothness values the Bessel term simplifies to polynomials in $r$ times exponentials, giving:
\subsubsection*{$\nu=\tfrac12$}
\[
K\!\left((x,c_i),(x',c_j)\right)
=
\sigma^{2}\,
\frac{1}{\big(1 + a^{2} d_{ij}^{2}\big)^{\frac{1}{2}+\frac{p}{2}}}
\exp\!\left(
-\,\frac{r}{\ell\,\sqrt{\,1 + a^{2} d_{ij}^{2}\,}}
\right).
\]
\subsubsection*{$\nu=\tfrac32$}
\[
K\!\left((x,c_i),(x',c_j)\right)
=
\sigma^{2}\,
\frac{1}{\big(1 + a^{2} d_{ij}^{2}\big)^{\frac{3}{2}+\frac{p}{2}}}
\left(
1 + \frac{\sqrt{3}\, r}{\ell\,\sqrt{\,1 + a^{2} d_{ij}^{2}\,}}
\right)
\exp\!\left(
-\,\frac{\sqrt{3}\, r}{\ell\,\sqrt{\,1 + a^{2} d_{ij}^{2}\,}}
\right).
\]
\subsubsection*{$\nu=\tfrac52$}
\[
K\!\left((x,c_i),(x',c_j)\right)
=
\sigma^{2}\,
\frac{1}{\big(1 + a^{2} d_{ij}^{2}\big)^{\frac{5}{2}+\frac{p}{2}}}
\left(
1 + \frac{\sqrt{5}\, r}{\ell\,\sqrt{\,1 + a^{2} d_{ij}^{2}\,}}
+ \frac{5\, r^{2}}{3\,\ell^{2}\,\big(1 + a^{2} d_{ij}^{2}\big)}
\right)
\exp\!\left(
-\,\frac{\sqrt{5}\, r}{\ell\,\sqrt{\,1 + a^{2} d_{ij}^{2}\,}}
\right).
\]
These closed forms make it straightforward to evaluate gradients of $\ell$, $\sigma$, $a$, or $\nu$ without resorting to special-function libraries.

\subsection{Positive Definiteness}
Let $\alpha=\nu+\tfrac{p}{2}$, $\lambda=\sqrt{2\nu}/\ell$, and write the kernel spectrum via Bochner’s theorem as $K\big((x,c_i),(x',c_j)\big)=\int_{\mathbb{R}^p} e^{i\omega^\top(x-x')}\,S_{ij}(\omega)\,d\omega$. Equation~\eqref{eq:mggp_matern_kernel} yields spectral matrices of the form
\[
S_{ij}(\omega)
= \sigma^2 C_{\nu,p}\;
(1+a^2 d_{ij}^2)^{-\alpha}
\Big(\,\|\omega\|^2+\frac{\lambda^2}{1+a^2 d_{ij}^2}\Big)^{-\alpha}
=:~\Phi_\omega(a^2 d_{ij}^2),
\]
with $C_{\nu,p}$ collecting constants independent of $i,j$. For each fixed $\omega$, it suffices to show that the matrix $[\Phi_\omega(a^2 d_{ij}^2)]_{ij}$ is positive semidefinite. Noting the identity
\[
(1+t)^{-\alpha}\Big(\|\omega\|^2+\frac{\lambda^2}{1+t}\Big)^{-\alpha}
=\big(\|\omega\|^2(1+t)+\lambda^2\big)^{-\alpha},
\]
and the fact that $x\mapsto x^{-\alpha}$ is completely monotone on $(0,\infty)$ for $\alpha>0$, we see that $\Phi_\omega(t)$ is completely monotone on $[0,\infty)$. Schoenberg’s theorem then implies $[\Phi_\omega(a^2 d_{ij}^2)]_{ij}\succeq 0$ for every $\omega$, and Bochner’s theorem guarantees that the spatial kernel in Eq.~\eqref{eq:mggp_matern_kernel} is positive definite for any nonnegative group distance matrix $[d_{ij}]$. When $\nu\rightarrow\infty$ and $a=0$ the kernel collapses to the separable RBF form, demonstrating that this MGGP Matérn strictly generalizes prior constructions while retaining probabilistic validity.

\section{Locally Conditioned KL Approximation}\label{subsec:lc-kl}

We approximate the KL divergence between the variational distribution \( q(U) \) and the prior \( p(U) \) using a locally conditional chain rule factorization. Let \( U = [U_1, \dots, U_M] \) be the collection of inducing variables.






We begin by expanding the $p(U)$ prior using the chain rule:
\begin{align*}
p(U) &= p(U_M | U_{1, \dots, M-1}) p(U_{1, \dots, M-1}) \\
&= p(U_M | U_{1, \dots, M-1}) p(U_{M-1} | U_{1, \dots, M-2}) p(U_{1, \dots, M-2}) \\
&= p(U_1) \prod_{j=2}^{M} p(U_j | U_{1, \dots, j-1}).
\end{align*}

Under the local conditional approximation, this becomes:
\[
p(U) \approx \prod_{j=1}^{M} p(U_j | U_{n(j)}).
\]

Then, the variational distribution $q(U)$ can be expanded similarly:

\begin{align*}
q(U) &= q(U_1) \prod_{j=2}^{M} q(U_j | U_{1, \dots, j-1})\\
&\approx  \prod_{j=1}^{M} q(U_j | U_{n(j)})
\end{align*}



The KL divergence between $q(U)$ and $p(U)$ is given by:

\[
\mathrm{KL}(q(U) \| p(U)) = \mathbb{E}_{q(U)} \left[ \log \frac{q(U)}{p(U)} \right]
\]

Next, the log ratio can be expanded and approximated as the product of $M$ terms:

\[
\mathbb{E}_{q(U)} \left[ \log \frac{q(U)}{p(U)} \right] \approx \mathbb{E}_{q(U)} \left[ \log \prod_{j=1}^{M} \frac{q(U_j | U_{n(j)})}{p(U_j | U_{n(j)})} \right]
\]

Becoming a linear sum of expectations:

\[
= \mathbb{E}_{q(U)} \left[ \sum_{j=1}^{M} \log \frac{q(U_j | U_{n(j)})}{p(U_j | U_{n(j)})} \right]
\]

\[
= \sum_{j=1}^{M} \mathbb{E}_{q(U)} \left[ \log \frac{q(U_j | U_{n(j)})}{p(U_j | U_{n(j)})} \right]
\]

We then apply \textbf{Law of total expectation} which gives:

\[
= \sum_{j=1}^{M} \mathbb{E}_{q(U_{n(j)})} \left[ \mathbb{E}_{q(U_j| U_{n(j)})} \left[ \log \frac{q(U_j | U_{n(j)})}{p(U_j | U_{n(j)})} \right] \right]
\]

\subsection{Gaussian Conditional Forms}

We assume that both the prior and variational posterior distributions over
\(U = [U_1, \dots, U_M]\) are multivariate Gaussian. In particular, we write

\[
p(U) = \mathcal{N}(U \mid 0,\, K),
\qquad
q(U) = \mathcal{N}(U \mid m,\, S),
\]

where \(K\) and \(S\) denote the prior and variational covariance matrices, and
\(m = [m_1, \dots, m_M]\) is the variational mean. Here we assume a zero-mean prior.

The conditional prior is given by:

\[
p(U_j | U_{n(j)}) = \mathcal{N}\left( K_{jn(j)} K_{n(j)n(j)}^{-1} U_{n(j)}, \; k_{jj} - K_{jn(j)} K_{n(j)n(j)}^{-1} K_{n(j)j} \right)
\]

The conditional variational distribution is given by:

\[
q(U_j | U_{n(j)}) = \mathcal{N}\left( m_j + S_{jn(j)} S_{n(j)n(j)}^{-1} (U_{n(j)} - m_{n(j)}), \; s_{jj} - S_{jn(j)} S_{n(j)n(j)}^{-1} S_{n(j)j} \right)
\]

\text{This reduces to }
$q(U_j) = \mathcal{N}(m_j, s_{jj})$ if  $s_{jj}$ is diagonal.\\ 

For notational compactness in subsequent expressions, we define these conditionals:

\begin{align*}
p(U_j | U_{n(j)}) &= \mathcal{N}\left( \beta_j^\top U_{n(j)}, \; \tau_j^2 \right) \\
q(U_j | U_{n(j)}) &= \mathcal{N}\left( m_j + \alpha_j^\top (U_{n(j)} - m_{n(j)}), \; \tilde{\tau}_j^2 \right)
\end{align*}

Where:

\begin{align*}
\beta_j^\top &= K_{jn(j)} K_{n(j)n(j)}^{-1}, \quad
\tau_j^2 = k_{jj} - K_{jn(j)} K_{n(j)n(j)}^{-1} K_{n(j)j} \\
\alpha_j^\top &= S_{jn(j)} S_{n(j)n(j)}^{-1}, \quad
\tilde{\tau}_j^2 = s_{jj} - S_{jn(j)} S_{n(j)n(j)}^{-1} S_{n(j)j}
\end{align*}

The KL divergence between two univariate Gaussians is:

\begin{align*}
\mathrm{KL}(q \,\|\, p)
&= \frac{1}{2} \left[
\log \frac{s_p^2}{s_q^2}
+ \frac{s_q^2}{s_p^2}
+ \frac{(\mu_q - \mu_p)^2}{s_p^2}
- 1
\right].
\end{align*}


The KL between $q(U_j | U_{n(j)})$ and $p(U_j | U_{n(j)})$ is given by:

\begin{align*}
\mathrm{KL}\!\left(q(U_j \mid U_{n(j)}) \,\|\, p(U_j \mid U_{n(j)})\right)
&= \frac{1}{2} \Bigg[
\log \frac{\tau_j^2}{\tilde{\tau}_j^2}
+ \frac{\tilde{\tau}_j^2}{\tau_j^2}
+ \frac{\big(m_j + \alpha_j^\top (U_{n(j)} - m_{n(j)}) - \beta_j^\top U_{n(j)}\big)^2}{\tau_j^2}
- 1 \Bigg].
\end{align*}


Then we calculate the expectation of this KL under $q(U_{n(j)})$:

\[
\Rightarrow \mathbb{E}_{q(U_{n(j)})} \left[ \mathrm{KL} \left( q(U_j | U_{n(j)}) \| p(U_j | U_{n(j)}) \right) \right]
\]

\begin{align*}
= \frac{1}{2} \left[
\log \frac{\tau_j^2}{\tilde{\tau}_j^2}
+ \frac{\tilde{\tau}_j^2}{\tau_j^2}
+ \frac{\mathbb{E}_{q(U_{n(j)})}\!\left[
\left(m_j + \alpha_j^\top (U_{n(j)} - m_{n(j)}) - \beta_j^\top U_{n(j)}\right)^2
\right]}{\tau_j^2}
- 1 \right].
\end{align*}



We treat the mean term:
\[
\left(m_j + \alpha_j^\top (U_{n(j)} - m_{n(j)}) - \beta_j^\top U_{n(j)}\right)^2
\]

This is rewritten as:
\[
= \left((\alpha_j - \beta_j)^\top U_{n(j)} + (m_j - \alpha_j^\top m_{n(j)})\right)^2
\]

Using the following identity:
\[
\mathbb{E}_{x \sim \mathcal{N}(\mu, \Sigma)} \left[(a^\top x + b)^2\right] = a^\top \Sigma a + (a^\top \mu + b)^2
\]

Applying this gives:

\begin{align*}
\mathbb{E}_{q(U_{n(j)})}\!\left[
\left(m_j + \alpha_j^\top (U_{n(j)} - m_{n(j)}) - \beta_j^\top U_{n(j)}\right)^2
\right]
=
(\alpha_j - \beta_j)^\top S_{n(j)n(j)} (\alpha_j - \beta_j)
\\
\quad + \left[\,(\alpha_j - \beta_j)^\top m_{n(j)} + \left(m_j - \alpha_j^\top m_{n(j)}\right)\right]^2.
\end{align*}


This expectation can be expanded further and simplified as:

\[
= \alpha_j^\top S_{n(j)n(j)} \alpha_j + \beta_j^\top S_{n(j)n(j)} \beta_j - 2 \alpha_j^\top S_{n(j)n(j)} \beta_j + (\beta_j^\top m_{n(j)} - m_j)^2
\]

Substituting this into the KL formula yields the final approximation:



\begin{align*}
\mathrm{KL}(q(U) \| p(U)) \approx \sum_{j=1}^{M} \mathbb{E}_{q(U_{n(j)})} \left[ \mathbb{E}_{q(U_j| U_{n(j)})} \left[ \log \frac{q(U_j | U_{n(j)})}{p(U_j | U_{n(j)})} \right] \right]
\end{align*}


\begin{align*}
= \sum_{j=1}^{M} \frac{1}{2} \Bigg[
&\log \frac{\tau_j^2}{\tilde{\tau}_j^2}
+ \frac{\tilde{\tau}_j^2}{\tau_j^2}
+ \frac{\left(m_j - \beta_j^\top m_{n(j)}\right)^2}{\tau_j^2} \\
&\quad + \frac{\beta_j^\top S_{n(j)n(j)} \beta_j}{\tau_j^2}
+ \frac{\alpha_j^\top S_{n(j)n(j)} \alpha_j}{\tau_j^2}
- \frac{2 \alpha_j^\top S_{n(j)n(j)} \beta_j}{\tau_j^2}
- 1
\Bigg].
\end{align*}



\section{Scalable Variational MGGPs}\label{subsec41}
Combining the non-separable Matérn kernel with the locally conditioned KL approximation leads to the following algorithm:
\begin{enumerate}
    \item Initialize inducing points for each group via k-means++ over spatial coordinates and construct a hybrid spatial/group neighborhood graph.
    \item Optimize the ELBO with stochastic gradients, forming mini-batches from spatial patches that include multiple groups to expose cross-group interactions.
    \item Evaluate the localized KL divergence by traversing neighborhoods $\mathcal{N}(j)$, using sparse Cholesky updates derived from conditional covariance factors.
    \item Update Matérn hyperparameters $(\nu_c, \ell_c, \sigma_c)$ and the group covariance $\Gamma$ jointly under a low-rank parameterization that enforces $\Gamma \succeq 0$.
\end{enumerate}
Efficient neighborhood search (ball trees, cover trees, or LSH) keeps neighbor selection sub-quadratic, and whitening of the inducing variables improves numerical stability. The resulting method scales to hundreds of thousands of spatial locations per group while preserving cross-group fidelity, enabling seamless integration with the spatial factorization models in Chapter~\ref{chap:spatial-factorization}.

\section{Discussion}
The proposed inference scheme balances flexibility and scalability: the kernel supports heterogeneous spatial roughness and interpretable cross-group couplings, the locally conditioned KL objective preserves informative dependencies without dense-matrix penalties, and stochastic optimization integrates naturally with downstream hierarchical models. Limitations remain when inter-group dependencies are global rather than local or when certain groups contain few observations, in which case neighborhood graphs must be augmented with global factors. Nevertheless, this framework lays the foundation for the application chapters, namely Chapter~\ref{chap:spatial-factorization} on spatial factorization and Chapter~\ref{chap:chromatin} on 3D chromatin modeling.

\chapter{Spatial Factorization Methods}\label{chap:spatial-factorization}
\section{Dimensionality Reduction for Spatial Data}
Context on PCA, NMF, and related techniques before introducing Nonnegative Spatial Factorization (NSF) with GP priors.
\section{Multi-Group Extensions of Spatial Factorization}


Description of the MGGP-aware NSF extension and its coupling with the proposed variational inference framework.

\subsection{Nonnegative Multigroup Spatial Factorization Inference}\label{subsec42}

Given the variational strategy for variational MGGPs (refer Section~\ref{subsec41}), we need to define $q(U)$, $p(U)$, $q(F)$ and the likelihood term $p(Y|F)$ in order to train the model.

\subsubsection{Evidence Lower Bound (ELBO) Expectation term}\label{subsubsec621}

In order to compute the variational posterior $q(F)$, we need to first define the variational inducing point distribution $q(U)$ in order to perform marginalization over $U$.
We use the mean-field variational strategy for each of the inducing variational distributions:
\[ q(U) = \prod_{l}^L q(u^{l}) \]

\[ q(u^{l}) = \mathcal{N}(u^{l} | m_{u}^{l},  L_{u}^{l}L_{u}^{l^{T}}) \]

Similarly, the posterior distribution becomes a product of posterior factors:

\[ q(F|X, C_{x}; \theta) = \prod_{l}^L q(f^l|X, C_{x}; \theta^l) \]

\[ q(f^l|X, C_{x}; \theta^l) =  \int q(f^{l} | u^{l}, X, C_{x}, Z, C_{z}; \theta^l) q(u^{l}) \, du^{l} \]

\subsubsection{Gaussian Marginalization Identities}\label{subsecA1}
For computational efficiency and numerical stability, we use the whitening strategy for Gaussian Processes \cite{Matthews2017-hy}, which involves performing change of variables of $u^l$ to a variable $g^l$. Where \( u^{l} = L^{l}g^{l} \), and \( L^{l} \) satisfies \( L^{l}L^{l^{T}} = K_{zz}^{l} \):

\[ q(u^{l}) = q(g^{l}) \left| \frac{\partial g^{l}}{\partial u^{l}} \right| \]

where:

\[ q(g^{l}) = \mathcal{N}(g^{l} | m_{g}^{l}, L_{g}^{l}L_{g}^{l^{T}}) \]

and 

\[ \left| \frac{\partial g^{l}}{\partial u^{l}} \right| = |L^{l^{-1}}| \]

This leads to marginalize over \( g^{l} \) instead of \( u^{l} \):

\[ \int q(f^{l} | L^{l}g^{l}) q(g^{l}) \left| \frac{\partial g^{l}}{\partial u^{l}} \right| du^{l} =  \int q(f^{l} | L^{l}g^{l}) q(g^{l}) dg^{l} \]

This adds an extra \( L^{l} \) term to the mean:

\[  q(f^l|X, C_{x}; \theta^l) = \int \mathcal{N}(f^{l} | \alpha^{l} L^{l} g^{l}, Q_{f}^{l}) \mathcal{N}(g^{l} | m_{g}^{l}, L_{g}^{l}L_{g}^{l^{T}}) dg^{l}  \]

where:
\[\alpha^{l} = K_{xz}^{l} K_{zz}^{l^{-1}} \]
\[Q_{f}^{l} = K_{xx}^{l} - K_{xz}^{l} K_{zz}^{l^{-1}}K_{zx}^{l} \]

We solve for the marginalized mean and covariance using Gaussian marginalization.
\[ q(f^l|X, C_{x}; \theta^l) = \mathcal{N}(m_{f}^{l}, S_{f}^{l}) \]


\[ m_{f}^{l} = \alpha^{l} L^{l} m_{g}^{l} \]

\[ S_{f}^{l} = Q_{f}^{l} + \alpha^{l} L^{l} L_{g}^{l} L_{g}^{l^{T}} L^{l^{T}} \alpha^{l^{T}} \]

\[ \alpha^{l} L^{l} = K_{xz}^{l} K_{zz}^{l^{-1}} L^{l} \]

We can define this as follows:

\[ \hat{\alpha}^{l} = K_{xz}^{l} L^{l^{-T}} \]

which can be computed using forward substitution for lower triangular matrices:

\[ \hat{\alpha}^{l^{T}} = L^{l} \backslash K_{zx}^{l} \]

\[ S_{f}^{l} = K_{xx}^{l} - \hat{\alpha}^{l} \hat{\alpha}^{l^{T}} + \alpha^{l} L_{g}^{l} L_{g}^{l^{T}} \alpha^{l^{T}} \]

We can also define:

\[ \hat{L}_{g}^{l} = \alpha^{l} L_{g}^{l} \]

Since we only need the diagonal covariance for the posterior, we can compute the covariance using properties of diagonal matrix products:

\[ \text{diag}(S_{f}^{l}) = \text{diag}(K_{xx}^{l}) - \sum_{j} (\hat{\alpha}^{l}_{[:, j]})^{2} + \sum_{j} (\hat{L}_{g}^{l_{[:, j]}})^{2} \]

Making the posterior to be independent of inputs and components:

\[ q(F) = \prod_{l}^L \prod_{i}^N q(f_{il}) \]

For the hybrid model also define variational distribution over $H$.

\[ q(H) = \prod_{t}^T \prod_{i}^N q(h_{it}) \]

We sample from these distributions.

\[f^{s}_{il} \sim q(f_{il})\]
\[h^{s}_{it} \sim q(h_{it})\]

And construct the rate of the Poisson distribution.

\[\lambda^{s}_{ij} = \sum_{l=1}^L w_{jl} e^{f^{s}_{il}} + \sum_{t=1}^T v_{jt} e^{h^{s}_{it}}\]

We construct the log probability of the Poisson distribution. And construct the approximation of the expectaction term. We can drop the factorial terms containing $y_ij$ since they don't contribute to the gradient calculation.
\[ \int{\log{p(Y|F)} q(F)dF} \approx \frac{1}{S} \sum^{S}_{s}[y_{ij} \log \nu_i \lambda^{s}_{ij} - \nu_i \lambda^{s}_{ij}] \]


\chapter{Applications to Chromatin and 3D Genomics}\label{chap:chromatin}
\section{Modeling 3D Chromatin Structure}
Application of Deep GP ideas to infer 3D chromatin conformations from linear genomic measurements and Hi-C correlations.
\section{Flexible Likelihoods and Deep Gaussian Processes}
Demonstration of flexible likelihoods, kernels, and group structures enabled by the unified framework for large-scale genomic datasets.

\chapter{Conclusions}
Summary of contributions across theory, methodology, and applications, along with open questions for spatial genomics and MGGP research.
\appendix
\chapter{Gaussian Identities}\label{appendixA}
Here we define and derive Gaussian identities for multivariate normal distributions.

\section{Marginalization of a Linear--Gaussian Model}\label{appendixA1}

\paragraph{Setup and notation.}
Let $f\in\mathbb{R}^{N}$ and $u\in\mathbb{R}^{M}$ be random vectors.
Assume the conditional and prior distributions
\begin{align}
p(f\mid u) &= \mathcal{N}\!\big(f \mid M u + m,\, \Sigma_f\big), \label{eq:A1_cond}\\
p(u)       &= \mathcal{N}\!\big(u \mid m_u,\, S\big), \label{eq:A1_prior}
\end{align}
where $M\in\mathbb{R}^{N\times M}$, $m\in\mathbb{R}^{N}$, and $m_u\in\mathbb{R}^{M}$.
The matrices $\Sigma_f\succeq 0$ and $S\succeq 0$ denote covariance terms.
We derive the marginal $p(f)$ and its moments in detail.

\subsection*{A1.1 Laws of total expectation and covariance}
We make use of two standard results, with all expectations taken over $u$:
\begin{align}
\text{(Law of total expectation)}\quad &\mathbb{E}[f] = \mathbb{E}_u\!\big[\mathbb{E}[f\mid u]\big],\\
\text{(Law of total covariance)}\quad &\operatorname{Cov}(f)
= \mathbb{E}_u\!\big[\operatorname{Cov}(f\mid u)\big]
+ \operatorname{Cov}_u\!\big(\mathbb{E}[f\mid u]\big).
\end{align}

\subsection*{A1.2 Marginal mean of $f$}
From \eqref{eq:A1_cond}, the conditional mean of $f$ is
\[
\mathbb{E}[f\mid u] = M u + m.
\]
Applying the law of total expectation step by step:
\begin{align}
\mathbb{E}[f]
&= \mathbb{E}_u\!\big[\mathbb{E}[f\mid u]\big] \nonumber\\[4pt]
&= \mathbb{E}_u[M u + m] \nonumber\\[4pt]
&= M\,\mathbb{E}_u[u] + \mathbb{E}_u[m] \nonumber\\[4pt]
&= M m_u + m.
\label{eq:A1_mean}
\end{align}

\subsection*{A1.3 Marginal covariance of $f$}
The conditional covariance $\operatorname{Cov}(f\mid u)$ is fixed and equal to $\Sigma_f$, hence
\[
\mathbb{E}_u[\operatorname{Cov}(f\mid u)] = \Sigma_f.
\]
The second term of the law of total covariance expands as
\begin{align}
\operatorname{Cov}_u\!\big(\mathbb{E}[f\mid u]\big)
&= \operatorname{Cov}_u(Mu+m) \nonumber\\[4pt]
&= \operatorname{Cov}_u(Mu) \nonumber\\[4pt]
&= M\,\operatorname{Cov}_u(u)\,M^\top \nonumber\\[4pt]
&= M S M^\top.
\end{align}
Combining both components,
\begin{align}
\operatorname{Cov}(f)
= \Sigma_f + M S M^\top.
\label{eq:A1_cov}
\end{align}

\subsection*{A1.4 Marginal distribution}
A linear transformation of jointly Gaussian variables is itself Gaussian. 
Thus the marginal of $f$ is fully determined by its mean \eqref{eq:A1_mean} and covariance \eqref{eq:A1_cov}:
\begin{align}
p(f)
= \mathcal{N}\!\big(
f \mid M m_u + m,\,
\Sigma_f + M S M^\top
\big).
\label{eq:A1_marginal}
\end{align}


Equations~\eqref{eq:A1_mean}--\eqref{eq:A1_marginal} follow directly from
the laws of total expectation and covariance; see \cite{Damianou2015-dy}.



\section{Application to Gaussian Processes}\label{appendixA2}
For Gaussian processes, instantiate the prior over function values \(f\) and inducing variables \(u\) as
\begin{align}
p(f) &= \mathcal{N}\!\big(f \mid \mu_f,\, K_{xx}\big), \\
p(u) &= \mathcal{N}\!\big(u \mid \mu_u,\, K_{zz}\big),
\end{align}
with joint
\begin{align}
p(f,u) = \mathcal{N}\!\left(
\begin{bmatrix} f \\ u \end{bmatrix}
\Bigg|
\begin{bmatrix} \mu_f \\ \mu_u \end{bmatrix},
\begin{bmatrix}
K_{xx} & K_{xz} \\
K_{zx} & K_{zz}
\end{bmatrix}
\right).
\end{align}
Conditioning gives \cite{Holt2023-yf}
\begin{align}
p(f \mid u)
= \mathcal{N}\!\left(
f \mid \mu_f + K_{xz}K_{zz}^{-1}(u-\mu_u),\,
K_{xx} - K_{xz}K_{zz}^{-1}K_{zx}
\right).
\end{align}
Matching to \eqref{eq:A1_cond} identifies
\begin{align}
M &= K_{xz} K_{zz}^{-1}, \\
m &= \mu_f - M \mu_u, \\
\Sigma_f &= K_{xx} - K_{xz} K_{zz}^{-1} K_{zx}.
\end{align}
Substituting these into \eqref{eq:A1_marginal} yields the GP marginal implied by the inducing prior \(p(u)=\mathcal{N}(u\mid m_u,S)\):
\begin{align}
p(f)
= \mathcal{N}\!\left(
f \mid \mu_f + K_{xz}K_{zz}^{-1}(m_u-\mu_u),\;
K_{xx} + K_{xz}K_{zz}^{-1}(S-K_{zz})K_{zz}^{-1}K_{zx}
\right).
\end{align}
Setting \(S=K_{zz}\) and \(m_u=\mu_u\) recovers the original prior \(p(f)=\mathcal{N}(f\mid \mu_f, K_{xx})\), confirming consistency.






\bibliographystyle{plain}
\bibliography{references}
\end{document}
